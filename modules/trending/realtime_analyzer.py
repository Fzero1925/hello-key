#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æ—¶å…³é”®è¯åˆ†æå’Œçƒ­ç‚¹è¿½è¸ªç³»ç»Ÿ - Realtime Trending Keywords Analyzer
é’ˆå¯¹æ¬§ç¾æ—¶åŒºä¼˜åŒ–ï¼Œ12å°æ—¶å†…çƒ­ç‚¹æ£€æµ‹ï¼Œå•†ä¸šä»·å€¼å®æ—¶è¯„ä¼°

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å®æ—¶Google Trendsåˆ†æï¼ˆæ¯2å°æ—¶æ›´æ–°ï¼‰
2. æ¬§ç¾æ—¶åŒºä¼˜åŒ–çš„çƒ­ç‚¹æ£€æµ‹
3. å•†ä¸šä»·å€¼å’Œç«äº‰åº¦å³æ—¶è¯„ä¼°  
4. çƒ­ç‚¹è¯é¢˜è‡ªåŠ¨è§¦å‘æ–‡ç« ç”Ÿæˆ
5. å¤šæ•°æ®æºäº¤å‰éªŒè¯ï¼ˆReddit, YouTube, Amazonï¼‰
"""

import os
import sys
import codecs
import time
import json
import requests
import asyncio
import aiohttp
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Tuple, Optional, Any
import logging
from dataclasses import dataclass, asdict
import pytz
import random
import yaml

# Import v2 scoring functions
try:
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'keyword_tools'))
    from scoring import opportunity_score, estimate_value, estimate_adsense, estimate_amazon, make_revenue_range
except ImportError:
    # Fallback implementation
    def estimate_value(search_volume, opp, ads_params=None, aff_params=None, mode='max'):
        ads_params = ads_params or {"ctr_serp":0.25, "click_share_rank":0.35, "rpm_usd":10}
        aff_params = aff_params or {"ctr_to_amazon":0.12, "cr":0.04, "aov_usd":80, "commission":0.03}
        pv = search_volume * ads_params["ctr_serp"] * ads_params["click_share_rank"]
        ra = (pv/1000.0) * ads_params["rpm_usd"]
        rf = (search_volume*aff_params["ctr_to_amazon"])*aff_params["cr"]*aff_params["aov_usd"]*aff_params["commission"]
        base = max(ra, rf)
        return base * (0.6 + 0.4*opp/100.0)
    
    def make_revenue_range(v):
        return {"point": v, "range": f"${v*0.75:.0f}â€“${v*1.25:.0f}/mo"}

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

# æ—¶åŒºé…ç½®
US_EASTERN = pytz.timezone('US/Eastern')
US_PACIFIC = pytz.timezone('US/Pacific')  
UK_LONDON = pytz.timezone('Europe/London')
CHINA_TIMEZONE = pytz.timezone('Asia/Shanghai')

@dataclass
class TrendingTopic:
    """å®æ—¶çƒ­ç‚¹è¯é¢˜æ•°æ®ç»“æ„"""
    keyword: str
    category: str
    trend_score: float
    commercial_value: float
    search_volume_est: int
    competition_level: str
    urgency_score: float  # ç´§æ€¥åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰
    sources: List[str]
    time_detected: datetime
    peak_regions: List[str]
    related_terms: List[str]
    business_reasoning: str
    content_angle: str
    estimated_revenue: str
    social_signals: Dict[str, int]


@dataclass
class MarketOpportunity:
    """å¸‚åœºæœºä¼šåˆ†æ"""
    keyword: str
    opportunity_score: float  # 0-1
    competition_gap: str
    revenue_potential: str
    time_sensitivity: str  # "URGENT", "HIGH", "MEDIUM", "LOW"
    recommended_action: str
    content_strategy: str


class RealtimeTrendingAnalyzer:
    """å®æ—¶çƒ­ç‚¹åˆ†æå™¨ - é’ˆå¯¹æ™ºèƒ½å®¶å±…å¸‚åœºä¼˜åŒ–"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.data_dir = "data/realtime_trends"
        self.cache_dir = "data/trend_cache"
        self.trends_history = "data/trends_history"
        
        # Load v2 configuration
        self.v2_config = self._load_v2_config()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        for directory in [self.data_dir, self.cache_dir, self.trends_history]:
            os.makedirs(directory, exist_ok=True)
        
        # æ™ºèƒ½å®¶å±…ç›¸å…³çš„ç§å­å…³é”®è¯ - æ‰©å±•ç‰ˆæœ¬
        self.smart_home_seeds = {
            'smart_plugs': [
                'smart plug', 'wifi outlet', 'alexa plug', 'smart switch',
                'energy monitoring plug', 'outdoor smart plug', 'voice control outlet'
            ],
            'security_devices': [
                'security camera', 'video doorbell', 'smart lock', 'doorbell cam',
                'wireless camera', 'outdoor camera', 'home security system'
            ],
            'cleaning_devices': [
                'robot vacuum', 'robotic cleaner', 'smart mop', 'pet hair vacuum',
                'self emptying vacuum', 'mapping vacuum', 'quiet robot vacuum'
            ],
            'climate_control': [
                'smart thermostat', 'wifi thermostat', 'nest thermostat',
                'programmable thermostat', 'energy saving thermostat'
            ],
            'lighting': [
                'smart bulb', 'color changing bulb', 'wifi light', 'smart dimmer',
                'led smart bulb', 'outdoor smart lights', 'motion sensor lights'
            ],
            'speakers_displays': [
                'smart speaker', 'alexa echo', 'google nest', 'smart display',
                'voice assistant', 'smart home hub', 'wifi speaker'
            ],
            'emerging_categories': [
                'smart mirror', 'smart doorbell', 'smart garage door', 'smart garden',
                'smart pet feeder', 'smart air purifier', 'smart blinds'
            ]
        }
        
        # å•†ä¸šæ„å›¾å…³é”®è¯
        self.commercial_signals = [
            'best', 'review', 'buy', 'deal', 'sale', 'price', 'cheap',
            'discount', 'compare', 'vs', '2025', 'guide', 'how to choose'
        ]
        
        # ç´§æ€¥åº¦ä¿¡å·è¯
        self.urgency_signals = {
            'breaking': 1.0,
            'new': 0.9,
            'latest': 0.8,
            'trending': 0.9,
            'viral': 1.0,
            'hot': 0.8,
            'popular': 0.7,
            'rising': 0.8
        }
        
    def _load_v2_config(self) -> Dict:
        """Load Keyword Engine v2 configuration from YAML file"""
        config_path = "keyword_engine.yml"
        default_config = {
            "window_recent_ratio": 0.3,
            "thresholds": {"opportunity": 70, "search_volume": 10000, "urgency": 0.8},
            "weights": {"T": 0.35, "I": 0.30, "S": 0.15, "F": 0.20, "D_penalty": 0.6},
            "adsense": {"ctr_serp": 0.25, "click_share_rank": 0.35, "rpm_usd": 10},
            "amazon": {"ctr_to_amazon": 0.12, "cr": 0.04, "aov_usd": 80, "commission": 0.03},
            "mode": "max"
        }
        
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    # Merge with defaults
                    for key, value in default_config.items():
                        if key not in config:
                            config[key] = value
                        elif isinstance(value, dict):
                            for subkey, subvalue in value.items():
                                if subkey not in config[key]:
                                    config[key][subkey] = subvalue
                    return config
        except Exception as e:
            self.logger.warning(f"Could not load v2 config: {e}, using defaults")
        
        return default_config

    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('data/realtime_trends.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def get_optimal_analysis_time(self) -> Dict[str, Any]:
        """è·å–æ¬§ç¾æ—¶åŒºçš„æœ€ä½³åˆ†ææ—¶é—´"""
        now_utc = datetime.now(timezone.utc)
        
        # è½¬æ¢åˆ°ä¸»è¦æ—¶åŒº
        us_east = now_utc.astimezone(US_EASTERN)
        us_west = now_utc.astimezone(US_PACIFIC)
        uk_time = now_utc.astimezone(UK_LONDON)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ¬§ç¾çš„æ´»è·ƒæ—¶é—´ï¼ˆæ—©ä¸Š6ç‚¹-æ™šä¸Š10ç‚¹ï¼‰
        active_zones = []
        
        if 6 <= us_east.hour <= 22:
            active_zones.append(('US_East', us_east.hour))
        if 6 <= us_west.hour <= 22:
            active_zones.append(('US_West', us_west.hour))
        if 6 <= uk_time.hour <= 22:
            active_zones.append(('UK', uk_time.hour))
        
        # è®¡ç®—æœ€ä½³åˆ†ææƒé‡
        analysis_weight = len(active_zones) / 3.0
        is_prime_time = analysis_weight >= 0.5
        
        return {
            'timestamp': now_utc.isoformat(),
            'active_zones': active_zones,
            'analysis_weight': analysis_weight,
            'is_prime_time': is_prime_time,
            'us_east_hour': us_east.hour,
            'us_west_hour': us_west.hour,
            'uk_hour': uk_time.hour,
            'recommendation': 'ANALYZE' if is_prime_time else 'WAIT'
        }
    
    async def analyze_realtime_trends(self, force_analysis: bool = False) -> List[TrendingTopic]:
        """å®æ—¶åˆ†æå½“å‰çƒ­é—¨è¶‹åŠ¿"""
        timing_info = self.get_optimal_analysis_time()
        
        if not force_analysis and not timing_info['is_prime_time']:
            self.logger.info(f"Not prime time for analysis. Weight: {timing_info['analysis_weight']}")
            return []
        
        self.logger.info(f"ğŸ”¥ Starting realtime trend analysis. Active zones: {timing_info['active_zones']}")
        
        trending_topics = []
        
        # å¹¶è¡Œåˆ†æå¤šä¸ªæ•°æ®æº
        tasks = [
            self._analyze_google_trends_realtime(),
            self._analyze_social_signals(),
            self._analyze_search_volume_spikes(),
            self._analyze_seasonal_opportunities()
        ]
        
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"Task {i} failed: {result}")
                elif isinstance(result, list):
                    trending_topics.extend(result)
                    
        except Exception as e:
            self.logger.error(f"Error in parallel analysis: {e}")
        
        # å»é‡å’Œæ’åº
        unique_topics = self._deduplicate_and_rank(trending_topics)
        
        # ä¿å­˜ç»“æœ
        self._save_trending_results(unique_topics, timing_info)
        
        return unique_topics[:10]  # è¿”å›å‰10ä¸ªæœ€çƒ­é—¨çš„
    
    async def _analyze_google_trends_realtime(self) -> List[TrendingTopic]:
        """åˆ†æGoogle Trendså®æ—¶æ•°æ®"""
        topics = []
        
        try:
            # æ¨¡æ‹ŸGoogle Trendsæ•°æ®ï¼ˆå®é™…éƒ¨ç½²æ—¶è¿æ¥çœŸå®APIï¼‰
            current_trends = await self._get_simulated_google_trends()
            
            for trend_data in current_trends:
                if self._is_smart_home_related(trend_data['query']):
                    topic = TrendingTopic(
                        keyword=trend_data['query'],
                        category=self._categorize_keyword(trend_data['query']),
                        trend_score=trend_data['score'],
                        commercial_value=self._calculate_commercial_value(trend_data['query']),
                        search_volume_est=trend_data['volume'],
                        competition_level=trend_data['competition'],
                        urgency_score=self._calculate_urgency(trend_data),
                        sources=['google_trends'],
                        time_detected=datetime.now(timezone.utc),
                        peak_regions=trend_data.get('regions', ['US', 'UK']),
                        related_terms=trend_data.get('related', []),
                        business_reasoning=self._generate_business_reasoning(trend_data),
                        content_angle=self._suggest_content_angle(trend_data['query']),
                        estimated_revenue=self._estimate_revenue_potential(trend_data),
                        social_signals=trend_data.get('social', {})
                    )
                    topics.append(topic)
                    
        except Exception as e:
            self.logger.error(f"Google Trends analysis failed: {e}")
        
        return topics
    
    async def _analyze_social_signals(self) -> List[TrendingTopic]:
        """åˆ†æç¤¾äº¤åª’ä½“ä¿¡å·"""
        topics = []
        
        # Redditçƒ­ç‚¹åˆ†æ
        reddit_trends = await self._get_reddit_trending()
        for trend in reddit_trends:
            if self._is_smart_home_related(trend['title']):
                topic = TrendingTopic(
                    keyword=self._extract_keyword_from_title(trend['title']),
                    category=self._categorize_keyword(trend['title']),
                    trend_score=min(trend['score'] / 1000, 1.0),
                    commercial_value=self._calculate_commercial_value(trend['title']),
                    search_volume_est=trend['score'] * 10,
                    competition_level='Medium',
                    urgency_score=min(trend['comments'] / 100, 1.0),
                    sources=['reddit'],
                    time_detected=datetime.now(timezone.utc),
                    peak_regions=['US', 'UK', 'CA'],
                    related_terms=[],
                    business_reasoning=f"Reddit engagement: {trend['score']} upvotes, {trend['comments']} comments",
                    content_angle=self._suggest_content_angle(trend['title']),
                    estimated_revenue="$200-500/month",
                    social_signals={'reddit_upvotes': trend['score'], 'reddit_comments': trend['comments']}
                )
                topics.append(topic)
        
        return topics
    
    async def _analyze_search_volume_spikes(self) -> List[TrendingTopic]:
        """åˆ†ææœç´¢é‡æ¿€å¢çš„å…³é”®è¯"""
        topics = []
        
        # æ¨¡æ‹Ÿæœç´¢é‡æ¿€å¢æ£€æµ‹
        spike_keywords = [
            {
                'keyword': 'smart plug energy monitoring',
                'current_volume': 25000,
                'baseline_volume': 15000,
                'spike_percentage': 67,
                'regions': ['US', 'UK', 'CA'],
                'trigger_event': 'Energy cost concerns trending'
            },
            {
                'keyword': 'robot vacuum black friday',
                'current_volume': 45000,
                'baseline_volume': 12000,
                'spike_percentage': 275,
                'regions': ['US', 'UK'],
                'trigger_event': 'Pre-holiday shopping surge'
            },
            {
                'keyword': 'outdoor security camera wireless',
                'current_volume': 32000,
                'baseline_volume': 20000,
                'spike_percentage': 60,
                'regions': ['US', 'CA', 'AU'],
                'trigger_event': 'Home security awareness increase'
            }
        ]
        
        for spike in spike_keywords:
            if spike['spike_percentage'] > 50:  # åªå…³æ³¨50%ä»¥ä¸Šçš„æ¿€å¢
                topic = TrendingTopic(
                    keyword=spike['keyword'],
                    category=self._categorize_keyword(spike['keyword']),
                    trend_score=min(spike['spike_percentage'] / 100, 1.0),
                    commercial_value=0.9,  # æœç´¢æ¿€å¢é€šå¸¸è¡¨ç¤ºé«˜å•†ä¸šä»·å€¼
                    search_volume_est=spike['current_volume'],
                    competition_level='Medium-High',
                    urgency_score=0.95,  # æ¿€å¢å…³é”®è¯å…·æœ‰é«˜ç´§æ€¥åº¦
                    sources=['search_volume_monitor'],
                    time_detected=datetime.now(timezone.utc),
                    peak_regions=spike['regions'],
                    related_terms=[],
                    business_reasoning=f"Search volume spiked {spike['spike_percentage']}% due to: {spike['trigger_event']}",
                    content_angle='Timely buying guide capitalizing on current interest surge',
                    estimated_revenue="$400-800/month",
                    social_signals={}
                )
                topics.append(topic)
        
        return topics
    
    async def _analyze_seasonal_opportunities(self) -> List[TrendingTopic]:
        """åˆ†æå­£èŠ‚æ€§æœºä¼š"""
        topics = []
        current_month = datetime.now().month
        
        # å­£èŠ‚æ€§å…³é”®è¯æ˜ å°„
        seasonal_keywords = {
            9: ['back to school smart home', 'dorm room automation'],  # September
            10: ['halloween smart lights', 'security camera motion'],  # October  
            11: ['black friday smart plugs', 'cyber monday deals'],    # November
            12: ['christmas smart lights', 'holiday automation'],      # December
            1: ['new year smart home', 'resolution automation'],       # January
            2: ['valentine smart lights', 'romantic automation'],      # February
            3: ['spring cleaning robot', 'outdoor camera setup'],     # March
            4: ['easter smart decorations', 'garden automation'],     # April
            5: ['mother day smart home', 'gift automation'],          # May
            6: ['father day tech gifts', 'summer automation'],        # June
            7: ['summer outdoor smart', 'vacation security'],         # July
            8: ['back school smart tech', 'student automation']       # August
        }
        
        if current_month in seasonal_keywords:
            for keyword in seasonal_keywords[current_month]:
                topic = TrendingTopic(
                    keyword=keyword,
                    category=self._categorize_keyword(keyword),
                    trend_score=0.75,
                    commercial_value=0.85,
                    search_volume_est=8000 + random.randint(2000, 8000),
                    competition_level='Low-Medium',
                    urgency_score=0.8,
                    sources=['seasonal_analysis'],
                    time_detected=datetime.now(timezone.utc),
                    peak_regions=['US', 'UK', 'CA'],
                    related_terms=[],
                    business_reasoning=f"Seasonal opportunity for {current_month:02d} month - lower competition with targeted demand",
                    content_angle='Seasonal buying guide with timely recommendations',
                    estimated_revenue="$300-600/month",
                    social_signals={}
                )
                topics.append(topic)
        
        return topics
    
    def _is_smart_home_related(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸æ™ºèƒ½å®¶å±…ç›¸å…³"""
        text_lower = text.lower()
        
        # æ™ºèƒ½å®¶å±…ç›¸å…³æœ¯è¯­
        smart_home_terms = [
            'smart', 'wifi', 'bluetooth', 'alexa', 'google', 'nest',
            'automation', 'iot', 'connected', 'wireless', 'app control',
            'voice control', 'home assistant', 'smart home', 'robot',
            'security camera', 'doorbell', 'thermostat', 'plug', 'bulb',
            'speaker', 'display', 'hub', 'sensor', 'monitor'
        ]
        
        return any(term in text_lower for term in smart_home_terms)
    
    def _categorize_keyword(self, keyword: str) -> str:
        """ä¸ºå…³é”®è¯åˆ†ç±»"""
        keyword_lower = keyword.lower()
        
        category_mapping = {
            'smart_plugs': ['plug', 'outlet', 'switch'],
            'security_devices': ['camera', 'doorbell', 'lock', 'security'],
            'cleaning_devices': ['vacuum', 'robot', 'cleaner', 'mop'],
            'climate_control': ['thermostat', 'temperature', 'heating', 'cooling'],
            'lighting': ['bulb', 'light', 'lamp', 'dimmer', 'led'],
            'speakers_displays': ['speaker', 'echo', 'display', 'assistant'],
            'general_smart_home': ['smart home', 'automation', 'hub', 'system']
        }
        
        for category, terms in category_mapping.items():
            if any(term in keyword_lower for term in terms):
                return category
        
        return 'general_smart_home'
    
    def _calculate_commercial_value(self, keyword: str) -> float:
        """è®¡ç®—å…³é”®è¯çš„å•†ä¸šä»·å€¼"""
        keyword_lower = keyword.lower()
        value = 0.5  # åŸºç¡€å€¼
        
        # å•†ä¸šæ„å›¾ä¿¡å·
        for signal in self.commercial_signals:
            if signal in keyword_lower:
                value += 0.1
        
        # å“ç‰Œå…³é”®è¯åŠ åˆ†
        brands = ['amazon', 'google', 'nest', 'alexa', 'philips', 'ring', 'arlo']
        if any(brand in keyword_lower for brand in brands):
            value += 0.2
        
        # å¹´ä»½å…³é”®è¯åŠ åˆ†ï¼ˆæ—¶æ•ˆæ€§ï¼‰
        if '2025' in keyword_lower:
            value += 0.15
        
        return min(value, 1.0)
    
    def _calculate_urgency(self, trend_data: Dict) -> float:
        """è®¡ç®—ç´§æ€¥åº¦è¯„åˆ†"""
        urgency = 0.5
        
        # åŸºäºå…³é”®è¯ä¸­çš„ç´§æ€¥ä¿¡å·
        query_lower = trend_data['query'].lower()
        for signal, weight in self.urgency_signals.items():
            if signal in query_lower:
                urgency = max(urgency, weight)
        
        # åŸºäºè¶‹åŠ¿è¯„åˆ†
        if trend_data['score'] > 0.8:
            urgency += 0.2
        
        return min(urgency, 1.0)
    
    def _generate_business_reasoning(self, trend_data: Dict) -> str:
        """ç”Ÿæˆå•†ä¸šæ¨ç†åˆ†æ"""
        query = trend_data['query']
        score = trend_data['score']
        volume = trend_data['volume']
        
        reasoning_templates = [
            f"High trend score ({score:.2f}) with {volume:,} estimated searches indicates strong market interest",
            f"Rising search volume ({volume:,}) suggests emerging opportunity with commercial potential",
            f"Trending keyword with {score:.2f} momentum score - ideal for content creation",
            f"Market demand signal: {volume:,} searches with {score:.2f} trend acceleration"
        ]
        
        return random.choice(reasoning_templates)
    
    def _suggest_content_angle(self, keyword: str) -> str:
        """å»ºè®®å†…å®¹è§’åº¦"""
        keyword_lower = keyword.lower()
        
        if any(word in keyword_lower for word in ['best', 'top', 'review']):
            return "Comprehensive buying guide with honest product comparisons"
        elif 'vs' in keyword_lower:
            return "Head-to-head product comparison with clear winner recommendation"
        elif any(word in keyword_lower for word in ['how', 'guide', 'setup']):
            return "Step-by-step tutorial with practical tips and troubleshooting"
        elif '2025' in keyword_lower:
            return "Future-focused guide with latest technology trends and predictions"
        else:
            return "In-depth product analysis with real-world testing and recommendations"
    
    def _estimate_revenue_potential(self, trend_data: Dict) -> str:
        """ä¼°ç®—æ”¶ç›Šæ½œåŠ› - ä½¿ç”¨v2ç²¾ç¡®æ¨¡å‹"""
        volume = trend_data.get('volume', 0)
        competition = trend_data.get('competition', 'Medium')
        
        # Calculate difficulty score from competition level
        difficulty_map = {'Low': 0.2, 'Medium': 0.5, 'High': 0.8}
        difficulty = difficulty_map.get(competition, 0.5)
        
        # Estimate basic opportunity score for revenue calculation
        # For trending analysis, assume moderate values for other factors
        estimated_opp_score = 100 * (1 - 0.6 * difficulty)  # Simplified calculation
        
        try:
            # Use v2 precise revenue estimation
            est_value = estimate_value(
                search_volume=volume,
                opp_score=estimated_opp_score,
                ads_params=self.v2_config['adsense'],
                aff_params=self.v2_config['amazon'],
                mode=self.v2_config['mode']
            )
            
            # Generate human-friendly range
            revenue_range = make_revenue_range(est_value)
            return revenue_range['range']
            
        except Exception as e:
            self.logger.warning(f"v2 revenue calculation failed: {e}, using fallback")
            # Fallback to original logic
            if volume > 20000 and competition == 'Low':
                return "$500-1000/month"
            elif volume > 15000 and competition == 'Medium':
                return "$300-600/month"
            elif volume > 10000:
                return "$200-400/month"
            else:
                return "$100-250/month"
    
    def _extract_keyword_from_title(self, title: str) -> str:
        """ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯"""
        # ç®€åŒ–çš„å…³é”®è¯æå–é€»è¾‘
        title_lower = title.lower()
        
        for category_keywords in self.smart_home_seeds.values():
            for keyword in category_keywords:
                if keyword in title_lower:
                    return keyword
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç§å­å…³é”®è¯ï¼Œè¿”å›å‰å‡ ä¸ªé‡è¦è¯
        words = title_lower.split()[:3]
        return ' '.join(words)
    
    def _deduplicate_and_rank(self, topics: List[TrendingTopic]) -> List[TrendingTopic]:
        """å»é‡å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº"""
        # æŒ‰å…³é”®è¯å»é‡
        unique_topics = {}
        for topic in topics:
            if topic.keyword not in unique_topics:
                unique_topics[topic.keyword] = topic
            else:
                # ä¿ç•™è¶‹åŠ¿è¯„åˆ†æ›´é«˜çš„
                if topic.trend_score > unique_topics[topic.keyword].trend_score:
                    unique_topics[topic.keyword] = topic
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        sorted_topics = sorted(
            unique_topics.values(),
            key=lambda t: (t.trend_score * 0.4 + t.commercial_value * 0.3 + t.urgency_score * 0.3),
            reverse=True
        )
        
        return sorted_topics
    
    def _save_trending_results(self, topics: List[TrendingTopic], timing_info: Dict):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results = {
            'timestamp': timestamp,
            'timing_info': timing_info,
            'topics_count': len(topics),
            'topics': [asdict(topic) for topic in topics]
        }
        
        filename = f"{self.data_dir}/trending_analysis_{timestamp}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ç”¨äºå¿«é€Ÿè®¿é—®
        simplified = [
            {
                'keyword': t.keyword,
                'category': t.category,
                'trend_score': t.trend_score,
                'commercial_value': t.commercial_value,
                'urgency_score': t.urgency_score,
                'business_reasoning': t.business_reasoning,
                'estimated_revenue': t.estimated_revenue
            }
            for t in topics[:5]
        ]
        
        with open(f"{self.data_dir}/current_trends.json", 'w', encoding='utf-8') as f:
            json.dump(simplified, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ“Š Saved {len(topics)} trending topics to {filename}")
    
    async def _get_simulated_google_trends(self) -> List[Dict]:
        """æ¨¡æ‹ŸGoogle Trendsæ•°æ®ï¼ˆå¼€å‘é˜¶æ®µä½¿ç”¨ï¼‰"""
        # å®é™…éƒ¨ç½²æ—¶è¿™é‡Œä¼šè¿æ¥çœŸå®çš„Google Trends API
        simulated_trends = [
            {
                'query': 'smart plug energy monitoring 2025',
                'score': 0.89,
                'volume': 24000,
                'competition': 'Medium',
                'regions': ['US', 'UK', 'CA'],
                'related': ['energy saving', 'wifi smart plug', 'alexa compatible'],
                'social': {'mentions': 145, 'sentiment': 0.8}
            },
            {
                'query': 'robot vacuum pet hair reviews',
                'score': 0.92,
                'volume': 31000,
                'competition': 'High',
                'regions': ['US', 'UK', 'AU'],
                'related': ['pet-friendly vacuum', 'automatic cleaning', 'smart mapping'],
                'social': {'mentions': 203, 'sentiment': 0.9}
            },
            {
                'query': 'wireless security camera solar',
                'score': 0.85,
                'volume': 18500,
                'competition': 'Low-Medium',
                'regions': ['US', 'CA', 'AU'],
                'related': ['outdoor camera', 'battery powered', 'wireless security'],
                'social': {'mentions': 87, 'sentiment': 0.7}
            },
            {
                'query': 'smart thermostat nest vs ecobee',
                'score': 0.78,
                'volume': 15200,
                'competition': 'High',
                'regions': ['US', 'CA'],
                'related': ['smart climate control', 'energy efficiency', 'home automation'],
                'social': {'mentions': 164, 'sentiment': 0.8}
            },
            {
                'query': 'color changing smart bulbs alexa',
                'score': 0.81,
                'volume': 19800,
                'competition': 'Medium',
                'regions': ['US', 'UK', 'DE'],
                'related': ['rgb lighting', 'voice control', 'mood lighting'],
                'social': {'mentions': 119, 'sentiment': 0.9}
            }
        ]
        
        # æ·»åŠ éšæœºå˜åŒ–ä»¥æ¨¡æ‹Ÿå®æ—¶æ€§
        for trend in simulated_trends:
            trend['score'] += random.uniform(-0.1, 0.1)
            trend['volume'] += random.randint(-2000, 3000)
            trend['score'] = max(0.1, min(1.0, trend['score']))
            trend['volume'] = max(1000, trend['volume'])
        
        return simulated_trends
    
    async def _get_reddit_trending(self) -> List[Dict]:
        """è·å–Redditçƒ­é—¨è¯é¢˜ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰"""
        reddit_trends = [
            {
                'title': 'Best smart plug for energy monitoring in 2025?',
                'score': 342,
                'comments': 67,
                'subreddit': 'smarthome'
            },
            {
                'title': 'Robot vacuum recommendations for pet owners',
                'score': 598,
                'comments': 134,
                'subreddit': 'homeautomation'
            },
            {
                'title': 'Outdoor security camera setup without wiring',
                'score': 276,
                'comments': 45,
                'subreddit': 'homesecurity'
            }
        ]
        
        return reddit_trends
    
    def generate_market_opportunities(self, trending_topics: List[TrendingTopic]) -> List[MarketOpportunity]:
        """åŸºäºçƒ­é—¨è¯é¢˜ç”Ÿæˆå¸‚åœºæœºä¼šåˆ†æ"""
        opportunities = []
        
        for topic in trending_topics[:5]:  # åˆ†æå‰5ä¸ªçƒ­é—¨è¯é¢˜
            opportunity = MarketOpportunity(
                keyword=topic.keyword,
                opportunity_score=self._calculate_opportunity_score(topic),
                competition_gap=self._analyze_competition_gap(topic),
                revenue_potential=topic.estimated_revenue,
                time_sensitivity=self._determine_time_sensitivity(topic),
                recommended_action=self._recommend_action(topic),
                content_strategy=self._suggest_content_strategy(topic)
            )
            opportunities.append(opportunity)
        
        return opportunities
    
    def _calculate_opportunity_score(self, topic: TrendingTopic) -> float:
        """è®¡ç®—å¸‚åœºæœºä¼šè¯„åˆ†"""
        # ç»¼åˆè¯„åˆ†ï¼šè¶‹åŠ¿ * å•†ä¸šä»·å€¼ * (1 - ç«äº‰åº¦/5)
        competition_factor = {
            'Low': 0.1, 'Low-Medium': 0.25, 'Medium': 0.5, 
            'Medium-High': 0.75, 'High': 0.9
        }.get(topic.competition_level, 0.5)
        
        opportunity = (topic.trend_score * 0.4 + 
                      topic.commercial_value * 0.4 + 
                      (1 - competition_factor) * 0.2)
        
        return min(1.0, opportunity)
    
    def _analyze_competition_gap(self, topic: TrendingTopic) -> str:
        """åˆ†æç«äº‰ç¼ºå£"""
        if topic.competition_level in ['Low', 'Low-Medium']:
            return "Limited quality content available - excellent opportunity for authoritative guide"
        elif topic.competition_level == 'Medium':
            return "Moderate competition - focus on unique angle or superior depth"
        else:
            return "High competition - requires exceptional quality and unique positioning"
    
    def _determine_time_sensitivity(self, topic: TrendingTopic) -> str:
        """ç¡®å®šæ—¶é—´æ•æ„Ÿæ€§"""
        if topic.urgency_score > 0.8:
            return "URGENT"
        elif topic.urgency_score > 0.6:
            return "HIGH"
        elif topic.urgency_score > 0.4:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _recommend_action(self, topic: TrendingTopic) -> str:
        """æ¨èè¡ŒåŠ¨æ–¹æ¡ˆ"""
        if topic.urgency_score > 0.8 and topic.commercial_value > 0.7:
            return "IMMEDIATE CONTENT CREATION - High priority, immediate article generation recommended"
        elif topic.trend_score > 0.8:
            return "FAST TRACK - Create content within 24-48 hours to capitalize on trend"
        elif topic.commercial_value > 0.8:
            return "STRATEGIC CONTENT - Plan comprehensive guide to capture long-term value"
        else:
            return "MONITOR - Track trend development before content creation"
    
    def _suggest_content_strategy(self, topic: TrendingTopic) -> str:
        """å»ºè®®å†…å®¹ç­–ç•¥"""
        strategies = {
            'URGENT': "Quick publication with high-value insights, follow up with comprehensive guide",
            'HIGH': "Focus on trending angle while maintaining quality and authority",
            'MEDIUM': "Comprehensive analysis with unique positioning and superior depth",
            'LOW': "Evergreen content approach with focus on long-term SEO value"
        }
        
        time_sensitivity = self._determine_time_sensitivity(topic)
        return strategies.get(time_sensitivity, "Balanced approach with quality focus")


# ä¸»è¦æ¥å£å‡½æ•°
async def analyze_current_trends(force_analysis: bool = False) -> Dict[str, Any]:
    """åˆ†æå½“å‰è¶‹åŠ¿çš„ä¸»è¦æ¥å£"""
    analyzer = RealtimeTrendingAnalyzer()
    
    trending_topics = await analyzer.analyze_realtime_trends(force_analysis)
    market_opportunities = analyzer.generate_market_opportunities(trending_topics)
    
    return {
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'trending_topics': [asdict(topic) for topic in trending_topics],
        'market_opportunities': [asdict(opp) for opp in market_opportunities],
        'analysis_summary': {
            'total_topics': len(trending_topics),
            'urgent_topics': len([t for t in trending_topics if t.urgency_score > 0.8]),
            'high_commercial_value': len([t for t in trending_topics if t.commercial_value > 0.8]),
            'recommended_immediate_action': len([opp for opp in market_opportunities if 'IMMEDIATE' in opp.recommended_action])
        }
    }


# æµ‹è¯•å’Œæ¼”ç¤º
if __name__ == "__main__":
    async def main():
        print("[REALTIME] å®æ—¶å…³é”®è¯åˆ†æç³»ç»Ÿå¯åŠ¨...")
        
        # åˆ†æå½“å‰è¶‹åŠ¿
        results = await analyze_current_trends(force_analysis=True)
        
        print(f"\n[ANALYSIS] åˆ†æå®Œæˆï¼å‘ç° {results['analysis_summary']['total_topics']} ä¸ªçƒ­é—¨è¯é¢˜")
        print(f"[URGENT] ç´§æ€¥è¯é¢˜: {results['analysis_summary']['urgent_topics']} ä¸ª")
        print(f"[COMMERCIAL] é«˜å•†ä¸šä»·å€¼: {results['analysis_summary']['high_commercial_value']} ä¸ª")
        print(f"[ACTION] ç«‹å³è¡ŒåŠ¨å»ºè®®: {results['analysis_summary']['recommended_immediate_action']} ä¸ª")
        
        print("\n[TOP] Top 3 çƒ­é—¨è¯é¢˜:")
        for i, topic in enumerate(results['trending_topics'][:3]):
            print(f"{i+1}. {topic['keyword']}")
            print(f"   è¶‹åŠ¿è¯„åˆ†: {topic['trend_score']:.2f} | å•†ä¸šä»·å€¼: {topic['commercial_value']:.2f}")
            print(f"   æ¨ç†: {topic['business_reasoning']}")
            print()
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        with open('data/realtime_analysis_demo.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print("[SAVE] å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜åˆ° data/realtime_analysis_demo.json")
    
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())